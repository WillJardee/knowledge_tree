---
aliases: []
tags: [zettel]
projects: []
title: 
linter-yaml-title-alias: 
date created: Thursday, February 2nd 2023, 5:00:00 pm
date modified: Thursday, February 2nd 2023, 5:00:00 pm
---

# Fundamental Evolutionary Algorithms
```
Kruse, R., Borgelt, C., Braune, C., Mostaghim, S., & Steinbrecher, M. (2016). “Chapter 13: Fundamental
Evolutionary Algorithms,” Computational Intelligence: A Methodological Introduction (pages 245–297), 2nd
edition, Springer.
```

## A general Genetic Algorithm

****
**function** genalg ($f$: function, $\mu$: int, $p_x$: real, $p_m$: real): array of bit;
**begin**
	$t \leftarrow 0$;
	$\text{pop}(t) \leftarrow$ create a population of $\mu$ random bit strings;
	evaluate $\text{pop}(t)$ with the fitness function $f$
	**while** termination criterion is not fulfilled **do begin**
		$t \leftarrow t+1$;
		$\text{pop}(t)\leftarrow \emptyset$ 
		$\text{pop}^\prime \leftarrow$ select $\mu$ individuals $s_1, \cdots, s_\mu$ from $\text{pop}(t)$ with *roulette wheel selection*;
		**for** $i \leftarrow 1, \cdots , \mu/2$ **do begin**
			$u \leftarrow$ choose random number from $U([0,1])$;
			**if** $u \leq p_x$ **then** crossover\_1point$(s_{2i-1}, s_{2i})$; **end**
			mutate\_bits$(s_{2i-1}, p_m)$;
			mutate\_bits$(s_{2i}, p_m)$;
			$\text{pop}(t) \leftarrow \text{pop}(t)\cup\{s_{2i-1}, s_{2i}\}$;
		**end**
		evaluate $\text{pop}(t)$ with the fitness function $f$;
		environmental selection $(\text{pop}(t), \text{pop}(t-1))$;
	**end** **return** best individual from $\text{pop}(t)$;
**end**
****

## The Schema Theorem
**def** *Schema*: A schema $h$ is a character sting of length $L$ over the alphebet $\{0, 1, \ast\}$, that is, $h \in \{0, 1, \ast\}^L$. The character $\ast$ is called *wildcard character* or *don't-care symbol*.

**def** *Matching*: A (binary) chromosome $c \in \{0,1\}^L$ *matches* a schema $h \in \{0, 1, \ast\}^L$, written as $c\triangleleft h$, if and only if it coincides with $h$ at all positions where $h$ is 0 or 1. Potions at which $h$ is $\ast$ are not taken into account.

Holland - **implicit parallelism**: a chromosome corresponds with many schemata at the same time. 

**def** *Mean Relative Fitness*: The *mean relative fitness* of chromosomes that match schema $h$ in the population $\text{pop}(t)$ is $$f^{(t)}_\text{rel} (h) = \frac{\sum_{s\in \text{pop}(t), s\triangleleft h} f^{(t)}_\text{rel}(s)}{N(h, t)}$$
**def** *Defining Length of a Schema*: The *defining length* deflen$(h)$ of a schema $h$ is the difference between the position of the last and first non-$\ast$ in $h$. 

**def** *Order of a Schema*: The *order* ord$(h)$ of a schema $h$ is the numI ber of zeroes and ones in $h$, that is, ord$(h) = \#(h, 0) + \#(h, 1) = L - \#(h, \ast)$, where the operator $\#$ counts the number of occurrences of its second argument in its first.

**Schema Theorem (for bit mutation)**:
$$N(h, t+1) \geq N(h, t) \cdot \frac{\bar{f}_t(h)}{\bar{f}_t}\left(1 - p_x \frac{\text{deflen}(h)}{L-1}\left(1 - \frac{N(h, t)}{\mu}\cdot \frac{\bar{f}_t(h)}{\bar{f}_t}\right)\right)(1-p_m)^{\text{ord}(h)} \, .$$$$N(h, t+1) \geq N(h, t) \cdot g(h, t)$$if $g(h,t) > 1$, the number of matching chromosomes grows exponentially, if $g(h, t) < 1$, it decreases exponentially. 

**Building black hypothesis**: Schemata such then should have
- high fitness ($\bar{f}_t(h)/\bar{f}_t$)
- small defining length ($1 - p_x \text{deflen}(h)/(L-1)\cdots$)
- low order ($(1-p_m)^{\text{ord}(h)}$)

## Two-Armed Bandit Argument
given a limited number of trials, $N$, and two tasks with unknown payoffs, $\mu_1$ and $\mu_2$, and variances, $\sigma^2_1$ and $\sigma_2^2$. If we play one arm, presumably the better one, for $n$ trials it should match: $N-n \approx e^{cn}$ for some constant $c$. This is to say that amount of exploitation should grow exponentially with the number of trials. 

## Principle of Minimal Alphabets
"The larger the size of the alphabet, the more difficult it is to find meaningful schemata, because a schema is matched by a large number of chromosomes." $\longrightarrow$ we expect an alphabet of 2 (binary) to be optimal.

## Evolution Strategies
oldest form of an evolutionary algorithm.
Given $f: \mathbb{R}^n \rightarrow \mathbb{R}$
Find $\bf{x} = (x_1, \cdots, x_n)$ that maximizes/minimizes $f$

**Selection**: follows a strictly elite principle. (let $\mu$ be the number of parents and $\lambda$ the number of children)
	**plus strategy**: selection works on parents and children ($\mu + \lambda$)
	**comma strategy**: section works only on the children
general rule of thumb is a ration $\mu : \lambda = 1:7$
(1+1)-ES $\rightarrow$ hill climbing

**Bariance adaptation** 
$1/5$ success rule: if more than $1/5$ of the children are better than the parents, the variance should be increased, and vice-versa. 
****
**function** varadapt\_global$(\sigma, p_s, \theta, \alpha:\text{real})$: real: 
**begin**
	**if** $p_s > \theta$ **then return** $\sigma \cdot \alpha$;
	**if** $p_s < \theta$  **then return** $\sigma /\alpha$;
	**return** $\sigma$
**end**
****
Adaptive-ES: mutate the global variance after $k$ generations - very similar to simulated annealing. 
variance can be kept per a chromosome and mutated as well. 

Use a co-variance matrix to introduce oblique mutations
- apply with the "square root" as defined by the lower triangular matrix from Cholesky decomp
- eigen decomp


## Genetic Programming
"With genetic programming it is tried to evolve symbolic expression or even computer programs with certain properties."
$\mathcal{F}$: function symbols and operators
$\mathcal{T}$: terminal symbols

Learning a Boolean function, example:
$\mathcal{F} = \{$and, or , not, if, $\cdots$ then $\cdots$ else, $\cdots$$\}$
$\mathcal{T} = \{x_1, \cdots , x_n, 1, 0\}$
Learning a symbolic regression, example:
$\mathcal{F} = \{+, -, *, /, \sqrt{}, \sin, \cos, \log, \exp, \cdots\}$
$\mathcal{T} = \{x_1, \cdots, x_m\} \cup \mathbb{R}$

If your functions are not domain complete, you're going to need to work within constraints and/or introduce protected version of error-prone functions (e.g. $/ \rightarrow \frac{x_1}{\sqrt{1+x^2_2}}$)

### Process: 
#### Initialization
Random: recursively build a parse tree according to the rules up to a given depth or number of nodes
doing both grow and full gives the *initialized half and half*
doing a span of $1 \rightarrow$ max depth gives the *initialized ramped half and half*
#### Genetic Operators
Mutation: replaces sub expression/sub-tree with randomly created sub-expression
Crossover: exchanges two sub expressions/sub-trees
Mutation tends to be completely left out and only crossover used. 
Editing: general editing (i.e. replacing constant/tautolgies), special editing (i.e. De Morgan's)

Introns: complicated phenomena that show up in GP that don't actually do anything - think "junk DNA"

## Multi-objective Optimization

**Weighted Combination of Objectives**

**Pareto-Optimal Solutions**
**def** *Pareto optimality*: An element $s \in \Omega$ is called *Pareto-optimal* w.r.t. the objective functions $f_i$, $i = 1, \cdots , k$ if there does not exist any element $s^\prime \in \Omega$ for which the following two properties hold:$$f_i(s^\prime) \geq f_i (s)$$ for all $1 \leq i \leq k$ and there is an $i$ with $1 \leq i \leq k$ such that$$f_i(s^\prime)> f_i(s) \, .$$
*Vector Evaluated Genetic Algorithm*
*Non-dominated Sorting Genetic Algorithm-II*
*Strength Pareto evolutionary algorithm 2*


