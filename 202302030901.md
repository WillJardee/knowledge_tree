---
aliases: [AI Lecture Notes]
tags: [zettel, notes, AI]
projects: [Advanced-AI]
title: AI Lecture Notes
linter-yaml-title-alias: AI Lecture Notes 
date created: Friday, February 3rd 2023, 9:01:24 am
date modified: Friday, February 3rd 2023, 9:01:24 am
---

# AI Lecture Notes

****
## 02-03-2023

### Parse Tree GP
Talking about population initialization for parse-trees; genetic programming
- Set a depth limit
- Grow method - start at root and grow outward
	- At each note in the tree, select from NT ($\mathcal{F}$) with probability $P_{\mathcal{F}}$
	- If depth limit is reached, only select from T ($\mathcal{T}$)
- Full method - until reaching the depth limit, only select from $\mathcal{F}$, then at the depth limit select from $\mathcal{T}$ 
- Half-and-half method (50% grow, 50% full)
- Ramped half-and-half -> usually linear due to eventual code bloat

Fighting growing complexity:
- introduce size into fitness
- limit depth size of mutations
- increase prob of crossover for shallower
- editing - very hard

### Linear GP
Linear comes from the fact that the set of operations can be seen as *linear* array

Chromosome is a sequence of 3-op codes
- assume we have a set of registers $(a, b, c, \cdots)$
- op $r_1$ $r_2$ <- this should look just like assembly operations (that’s what we’re going for)
	- PLUS $b$ $c$
	- MULT $a$ $b$
	- PLUS $a$ $d$
	- MINUS $a$ $e$ 
- Assume a standard return register
- Struct: Header-Code-Footer-Return <- can only edit the Code

``` ad-note
John Coza Genetic programming
```

### Differential Evolution
- Real-valued optimization problems
- mutation first -> crossover
- mutation: donor vector

(gets name from difference vector)

Assume $d$ dimensions, $x \in \mathcal{C}$ s.t. $x \in \mathbb{R}^d$, where $\mathcal{C}$ is your population
- typically, set upper and lower bounds for each gene (not required)
- Initiate by selecting at random in the given range
- Select three members of $\mathcal{C}$, $\{x_1, x_2, x_3\}$,at random s.t. $x_1 \neq x_2 \neq x_3 \neq x_1$
	- Calculate $v_j \leftarrow x_j(1) + \beta \left(x_j(2) - x_j(3)\right)$, where $\beta$ is some scale vector  in $[0,2]$, this is the donor vector
****
## 02-10-2023
### Multi-Objective Optimization (MOO)
Let $\mathcal{S}\subseteq \mathbb{R}^n$ be an $n$-dimensional search space, where $\mathcal{F}\subseteq \mathcal{S}$ corresponds to the feasible region and $\mathcal{I}$ correspond to an infeasible region
let $\bf{x} = (x_1, \cdots, x_n)$ be a “decision vector”
we define a single objective function $f_k(x)$ as a mapping $f_k: \mathbb{R}^n \rightarrow \mathbb{R}$
let $\bf{f}(\bf{x}) = (f_1(\bf{x}), \cdots , f_m(\bf{x})) \in mathcal{O} \subseteq \mathbb{R}^m$ be an “objective vector” that contains $m$ objective functions, $f_1, \cdots, f_m$. 
the multi objective optimization problem can be defined as $$\begin{align} \text{minimize } \; &f(x)\\\text{subject to } \; & l_i \leq x_i \leq u_i \, , \, i = 1, \cdots , n_g+n_h\\& g_j(x_j)\leq 0 \, , \, j = 1, \cdots , n_g\\ & h_k(x_k)= 0 \, , \, k = n_g+1, \cdots , n_g+n_h
\end{align}$$

#### Pareto Front
Consider a set of objectives $\bf{f}$ and decision vectors $\bf{x}^\prime$ and $\bf{x}^{\prime\prime}$. we say $\bf{x}^\prime$ denominates $\bf{x}^{\prime\prime}$ iff the following hold:
- $f_i(\bf{x}^\prime) \leq f_i(x^{\prime\prime}) \, , \, \forall f_i \in \bf{f}$
- $\exists f_i \in \bf{f},$ s.t., $f_i(\bf{x}^\prime)< f_i(\bf{x^\prime})$ $\longleftarrow$ if we exclude this, we say the point is weakly dominated (makes weak Pareto optimal set)
A solution vector $\bf{x}^\prime$ is said to be Pareto optimal if there does not exist another solution vector $(bf{x}^\prime \neq \bf{x}^{\prime\prime})$ such that $\bf{x}^{\prime\prime}$ dominates $\bf{x}^\prime$. (this is assuming that these vectors come from teh feasible region)
The set of Pareto optimal solutions is $\mathcal{P} = \{x \in \mathcal{F} | \nexists \bf{x}^\prime \in \mathcal{F}, \bf{x}^\prime \prec \bf{x}\}$
given the set of objective functions, $\bf{f}$, then the Pareto front $\mathcal{P}\mathcal{F} \in \mathcal{O}$ is $$\mathcal{PF} = \{\bf{f} = (f_1(x), \cdots, f_n(x)) | \bf{x} \in \mathcal{P}\}$$
NSGA: Nondominated Sorting Genetic Algorithm

#### Weighted Aggregation Method
$$f(x) = \sum_{j=1}^m w_j \cdot f_j(x)$$ where $w_i \geq 0$, $\sum_{j=1}^m w_i = 1$. what weights? only one solution returned. approach is convex, but some of the $f_i$ might not be.
**approaches:**
1. random distribution of weights
	- maintain an archive for our Pareto set
	- Assign each individual (decision vector) a random weight$$\begin{align}w_{1,i}(t) & = U(0, n_p)/n_p\\w_{2,i}(t) & = 1-w_{1, i}(t)\end{align}$$
	- add best in generation to archive and remove any dominated solutions from archive.
2. put the weights on the individuals and not the functions
****