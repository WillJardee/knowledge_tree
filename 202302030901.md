---
aliases: [AI Lecture Notes]
tags: [zettel, notes, AI]
projects: [Advanced-AI]
title: AI Lecture Notes
linter-yaml-title-alias: AI Lecture Notes 
date created: Friday, February 3rd 2023, 9:01:24 am
date modified: Friday, February 3rd 2023, 9:01:24 am
---

# AI Lecture Notes

****
## 02-03-2023

### Parse Tree GP
Talking about population initialization for parse-trees; genetic programming
- Set a depth limit
- Grow method - start at root and grow outward
	- At each note in the tree, select from NT ($\mathcal{F}$) with probability $P_{\mathcal{F}}$
	- If depth limit is reached, only select from T ($\mathcal{T}$)
- Full method - until reaching the depth limit, only select from $\mathcal{F}$, then at the depth limit select from $\mathcal{T}$ 
- Half-and-half method (50% grow, 50% full)
- Ramped half-and-half -> usually linear due to eventual code bloat

Fighting growing complexity:
- introduce size into fitness
- limit depth size of mutations
- increase prob of crossover for shallower
- editing - very hard

### Linear GP
Linear comes from the fact that the set of operations can be seen as *linear* array

Chromosome is a sequence of 3-op codes
- assume we have a set of registers $(a, b, c, \cdots)$
- op $r_1$ $r_2$ <- this should look just like assembly operations (that’s what we’re going for)
	- PLUS $b$ $c$
	- MULT $a$ $b$
	- PLUS $a$ $d$
	- MINUS $a$ $e$ 
- Assume a standard return register
- Struct: Header-Code-Footer-Return <- can only edit the Code

``` ad-note
John Coza Genetic programming
```

### Differential Evolution
- Real-valued optimization problems
- mutation first -> crossover
- mutation: donor vector

(gets name from difference vector)

Assume $d$ dimensions, $x \in \mathcal{C}$ s.t. $x \in \mathbb{R}^d$, where $\mathcal{C}$ is your population
- typically, set upper and lower bounds for each gene (not required)
- Initiate by selecting at random in the given range
- Select three members of $\mathcal{C}$, $\{x_1, x_2, x_3\}$,at random s.t. $x_1 \neq x_2 \neq x_3 \neq x_1$
	- Calculate $v_j \leftarrow x_j(1) + \beta \left(x_j(2) - x_j(3)\right)$, where $\beta$ is some scale vector  in $[0,2]$, this is the donor vector
****
## 02-10-2023
### Multi-Objective Optimization (MOO)
Let $\mathcal{S}\subseteq \mathbb{R}^n$ be an $n$-dimensional search space, where $\mathcal{F}\subseteq \mathcal{S}$ corresponds to the feasible region and $\mathcal{I}$ correspond to an infeasible region
let $\bf{x} = (x_1, \cdots, x_n)$ be a “decision vector”
we define a single objective function $f_k(x)$ as a mapping $f_k: \mathbb{R}^n \rightarrow \mathbb{R}$
let $\bf{f}(\bf{x}) = (f_1(\bf{x}), \cdots , f_m(\bf{x})) \in mathcal{O} \subseteq \mathbb{R}^m$ be an “objective vector” that contains $m$ objective functions, $f_1, \cdots, f_m$. 
the multi objective optimization problem can be defined as $$\begin{align} \text{minimize } \; &f(x)\\\text{subject to } \; & l_i \leq x_i \leq u_i \, , \, i = 1, \cdots , n_g+n_h\\& g_j(x_j)\leq 0 \, , \, j = 1, \cdots , n_g\\ & h_k(x_k)= 0 \, , \, k = n_g+1, \cdots , n_g+n_h
\end{align}$$

#### Pareto Front
Consider a set of objectives $\bf{f}$ and decision vectors $\bf{x}^\prime$ and $\bf{x}^{\prime\prime}$. we say $\bf{x}^\prime$ denominates $\bf{x}^{\prime\prime}$ iff the following hold:
- $f_i(\bf{x}^\prime) \leq f_i(x^{\prime\prime}) \, , \, \forall f_i \in \bf{f}$
- $\exists f_i \in \bf{f},$ s.t., $f_i(\bf{x}^\prime)< f_i(\bf{x^\prime})$ $\longleftarrow$ if we exclude this, we say the point is weakly dominated (makes weak Pareto optimal set)
A solution vector $\bf{x}^\prime$ is said to be Pareto optimal if there does not exist another solution vector $(bf{x}^\prime \neq \bf{x}^{\prime\prime})$ such that $\bf{x}^{\prime\prime}$ dominates $\bf{x}^\prime$. (this is assuming that these vectors come from teh feasible region)
The set of Pareto optimal solutions is $\mathcal{P} = \{x \in \mathcal{F} | \nexists \bf{x}^\prime \in \mathcal{F}, \bf{x}^\prime \prec \bf{x}\}$
given the set of objective functions, $\bf{f}$, then the Pareto front $\mathcal{P}\mathcal{F} \in \mathcal{O}$ is $$\mathcal{PF} = \{\bf{f} = (f_1(x), \cdots, f_n(x)) | \bf{x} \in \mathcal{P}\}$$
NSGA: Nondominated Sorting Genetic Algorithm

#### Weighted Aggregation Method
$$f(x) = \sum_{j=1}^m w_j \cdot f_j(x)$$ where $w_i \geq 0$, $\sum_{j=1}^m w_i = 1$. what weights? only one solution returned. approach is convex, but some of the $f_i$ might not be.
**approaches:**
1. random distribution of weights
	- maintain an archive for our Pareto set
	- Assign each individual (decision vector) a random weight$$\begin{align}w_{1,i}(t) & = U(0, n_p)/n_p\\w_{2,i}(t) & = 1-w_{1, i}(t)\end{align}$$
	- add best in generation to archive and remove any dominated solutions from archive.
2. put the weights on the individuals and not the functions
****
## Lecture 02-17-2023

### Evolutionary Reinforcement Learning

### Learner Classifier Systems
Environment - precepts -> agent <- action - reward response

Bandit problems - exploration vs exploitation (diversity, elitism)

### Markov Decision Process
- 1950s Richard Bellman
Markov Decision Process (MDP): $M = \langle S, A, R, \mathcal{T}, \gamma\rangle$
$S$: set of states
$A$: set of actions
$R$: reward function
$\mathcal{T}$: $S\times A\times S^\prime \rightarrow \mathbb{R} \in [0,1] \sim P(s^\prime \mid s, a)$
$\gamma \in [0,1]$: discount factor (not in every formalism of MDP)
Learns a policy, $\pi$

Model-based learning: learns $R$ and $\mathcal{T}$
Model-free learning: doesn’t worry about learning $R$ or $\mathcal{T}$

#### Model-Based
$$V^\ast (s) = \max_{a\in A}\left[R(s,a)+\gamma\sum_{s^\prime \in S}P(s^\prime | s, a)V(s^\prime)\right] \qquad \text{Bellman Equation } (1)$$$$\pi^\ast(s) = \underbracket{\text{argmax}}_{a\in A}\left[R(s,a) + \gamma \sum_{s^\prime \in S}P(s^\prime | s, a)V(s^\prime)\right] \qquad \text{Bellman Equation } (2)$$
**Value Iteration**
**Policy Iteration** (provable convergence)
**Policy Gradient**

#### Model-free
**Q-learning**: $Q(s,a) \leftarrow Q(s,a) + \eta \left(\underbrace{r + \gamma \max_{a^\prime} Q(s^\prime, a^\prime)}_\text{Bellman Error} - Q(s,a)\right)$ (to make this termperal difference, $a\rightarrow t$ and $a^\prime \rightarrow t+1$). Off policy method because we choose $a$ s.t. it is not always chosen optimally
**SARSA (State Action Reward State Action)**: $Q(s,a) \leftarrow Q(s,a) + \eta \left(r + \gamma \max_{a^\prime} Q(s^\prime, a^\prime) - Q(s,a)\right)$. On policy version of Q-learning

SARSA is safer than Q-learning - cliff analysis. SARSA is slower on extreme ranges, comparable with moderate rewards.

Eligibility Trace: (look for SARSA($\lambda$)). A state-action pair is eligible for update - eligibility is decayed over time. 

****
## Lecture 02-22-2023
## Rule Based ERL (Evolutionary Reinforcement Learning)
opposed with replacing the rules with a NN, giving Deep ERL (most are Pittsburg Model)

Agent based approach
- Pittsburg Model: Each individual in the population is a complete solution
- Michigan Model (Holland): Each individual is a single rule - one agent

### Learning Classifier Systems (LCS) - Holland, Booker
- Accuracy Based
- Anticipatory: ZCS
- Strength-Based

Classifier: Rule
	Bit string that defines messages
	$\underbrace{b_0 b_1 \cdots b_n}_\text{message - condition} \rightarrow \underbrace{b_0^\prime b_1^\prime \cdots b_n^\prime}_\text{message - action}$
	$b_i \in \{0,1,\#\}$ (the $\#$ gives rise to schema theory)
each rule gets a strength attached to it 
Updated through a temporal difference process called “bucket brigade”
Population: Set of rules (Michigan Model)
GA: Fitness proportionate selection, standard genetic operators (one-pt cross over)

trigger: candidate to activate
fire: actually activated

**LCS**:
	Let $B(C, t)$ be a bid, proposing that classification $C$ first at time $t$.
	Let $s(C, t)$ be the strength of classifier $C$ at time $t$.
	Let $R(C)$ be the number of non-don’t cares in the condition part of $C$, divided by the length of the condition part of $C$.
	Let $b$ be the fraction of strength to be bid. 
	Let $a$ be the fraction of the total number of classifiers that actually fired.
	If a rule matches a message on the message list, it issues a bid$$B(C, t) = b\,R(C) \cdot s(S,t)$$If $C$ loses the competition, $s(C, t)$ is unchanged. If $C$ is selected to fire,$$s(C, t+1) = s(C, t)- B(C, t) + a\, B(C^\prime, t)$$where $C^\prime$ are the classifiers to come next. This looks like temporal differencing and bucket brigade


### Competitive Production Systems
uses profit sharing
SAMUEL Architecture 

****
## Lecture 02-27-2023
**Intro to theory** 
*No Free Lunch* | *Ugly Duckling* | *Schema* 

Search and optimization 
machine learning -> generalization
convex vs. non-convex
Assumptions: 
	inductive bias: 
	representation (what), preference (how)
No free lunch - there is no best set of assumptions for all problems
Bias matching

No free lunch. theorems in optimization, 1997, Wolpert and Macready
	Suppose we have two algorithms $A_1$, $A_2$. Consider the set of problems uniformly averaged over all target functions $\mathcal{F}$. Then, $E_{a_i}\left[l | \mathcal{F}, n\right] - E_{a_0}\left[l | \mathcal{F}, n\right] = 0$.
	Consider any fixed data set, $\mathcal{D}$, uniformly averaged over all target functions $\mathcal{F}$. Then $E_{a_1}\left[l | \mathcal{F}, n\right] - E_{a_2}\left[l | \mathcal{F}, n\right] = 0$. 
	Consider the set of problems uniformly averaged all possible priors $P(\mathcal{F})$. Then $E_{a_1}\left[l | n\right] - E_{a_2}\left[l | n\right] = 0$. 
	Consider any fixed data set, $\mathcal{D}$, uniformly averaged over all possible priors $P(\mathcal{F})$. Then $E_{a_1}\left[l|n\right] - E_{a_2}\left[l|n\right] = 0$. 

Ugly duckling:
	How do you measure similarity? really depends on what you want to measure

Schema:
	Sort, low-order, above average schemata receive exponentially increasing trials in subsequent generations of a genetic algorithm