---
aliases: [AI Lecture Notes]
tags: [zettel, notes, AI]
projects: [Advanced-AI]
title: AI Lecture Notes
linter-yaml-title-alias: AI Lecture Notes 
date created: Friday, February 3rd 2023, 9:01:24 am
date modified: Friday, February 3rd 2023, 9:01:24 am
---

# AI Lecture Notes

****
## 02-03-2023

### Parse Tree GP
Talking about population initialization for parse-trees; genetic programming
- Set a depth limit
- Grow method - start at root and grow outward
	- At each note in the tree, select from NT ($\mathcal{F}$) with probability $P_{\mathcal{F}}$
	- If depth limit is reached, only select from T ($\mathcal{T}$)
- Full method - until reaching the depth limit, only select from $\mathcal{F}$, then at the depth limit select from $\mathcal{T}$ 
- Half-and-half method (50% grow, 50% full)
- Ramped half-and-half -> usually linear due to eventual code bloat

Fighting growing complexity:
- introduce size into fitness
- limit depth size of mutations
- increase prob of crossover for shallower
- editing - very hard

### Linear GP
Linear comes from the fact that the set of operations can be seen as *linear* array

Chromosome is a sequence of 3-op codes
- assume we have a set of registers $(a, b, c, \cdots)$
- op $r_1$ $r_2$ <- this should look just like assembly operations (that’s what we’re going for)
	- PLUS $b$ $c$
	- MULT $a$ $b$
	- PLUS $a$ $d$
	- MINUS $a$ $e$ 
- Assume a standard return register
- Struct: Header-Code-Footer-Return <- can only edit the Code

``` ad-note
John Coza Genetic programming
```

### Differential Evolution
- Real-valued optimization problems
- mutation first -> crossover
- mutation: donor vector

(gets name from difference vector)

Assume $d$ dimensions, $x \in \mathcal{C}$ s.t. $x \in \mathbb{R}^d$, where $\mathcal{C}$ is your population
- typically, set upper and lower bounds for each gene (not required)
- Initiate by selecting at random in the given range
- Select three members of $\mathcal{C}$, $\{x_1, x_2, x_3\}$,at random s.t. $x_1 \neq x_2 \neq x_3 \neq x_1$
	- Calculate $v_j \leftarrow x_j(1) + \beta \left(x_j(2) - x_j(3)\right)$, where $\beta$ is some scale vector  in $[0,2]$, this is the donor vector
****
